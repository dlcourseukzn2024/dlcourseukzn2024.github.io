{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1C4NtTmdbFTG"},"source":["# Homework #2: Convolutional Neural Networks\n","Due date is on Canvas.\n","# You are required to upload a pdf of this notebook along with the ipynb file.\n","## Harvard ID:"]},{"cell_type":"markdown","metadata":{"id":"4ws6gRwkbGra"},"source":["## Question 1\n","(a) You have an input volume that is 15x15x8 and pad it using *p = 2*. What is the dimension of the resulting volume?"]},{"cell_type":"markdown","metadata":{"id":"3hNHcFZabKrm"},"source":["(b) You have an input volume that is 32x32x16 and apply max pooling with a stride of 2 and a filter of size of 2x2. What is the output volume?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ocMkI2zUbK0H"},"source":["(c) You have an input volume that is 63x63x16 and convolve it with 32 filters that are each 7x7, and a stride of 1. You want to use a \"same\" convolution. What is the padding *p*?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uPp2WoOnbK9R"},"source":["(d) You have an input volume that is 63x63x16 and convolve it with 32 filters that are each 7x7, using a stride of 2 and no padding. What is the output volume?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NAPjxJ20bTzJ"},"source":["## Question 2: Classification of Chest X-rays\n","Chest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.\n","\n","This [NIH Chest X-ray Dataset](https://www.kaggle.com/nih-chest-xrays/data) is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\n","\n","Here we will use a subset of the data, rather than all 112,120 images. The images have been split into training, validation and test sets, each with folders containing the images for a particular diagnosis (class). The full data set contains images with single labels and multi-lables, with a total of 15 unique diagnoses. Our subsample contains only single label images with a total of 7 diagnoses: atelectasis, effusion, infiltration, mass, nodule, none (no finding), and pneumothorax. Your task is to classify the images correctly by building multiple CNNs and comparing their performance.\n","\n","Here are what a few of the X-rays look like:\n","\n","![xrays](https://drive.google.com/uc?id=1m42zYcI1YWDn9TuBlA5vukN7rqaILuXF)\n","\n","### Load the data\n","\n","The data are available in Google Drive [here](https://drive.google.com/open?id=1Q9DGe2-WcN1T5ToDLr4AMk-UAFedb309) or in Dropbox [here](https://www.dropbox.com/sh/6rstxss79669361/AABXuOWEA75MtWWTVeER4UeKa?dl=0). **Please note that if you are accessing the Google Drive folder that there are 2 xray datasets: `xrays` and `xrays-small`. You will use the `xrays` dataset for this assignment.**\n","\n","Load the data and print the number of training, validation and test set examples there are of each class. Be sure to change the directory path provided below to your own data path."]},{"cell_type":"code","metadata":{"id":"M6hEcRu2bAxM"},"source":["import numpy as np\n","from sklearn.decomposition import PCA\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import datasets, layers, models\n","import os, shutil"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9Wv3n-Ebahd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzgw5B6Lbc6Z"},"source":["# Define directories of where the training, validation and test sets reside\n","# Heather's: \n","base_dir = 'drive/My Drive/Teaching/BST 261/2020/Colab Notebooks/In-class examples/Data/xrays/'\n","# This is the path to where my files are - your path will be different, something like this:\n","# base_dir = 'drive/My Drive/Data/xrays/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YofY1uEobdH-"},"source":["train_dir = os.path.join(base_dir, 'train_dir')\n","validation_dir = os.path.join(base_dir, 'validation_dir')\n","test_dir = os.path.join(base_dir, 'test_dir')\n","\n","# Training Data\n","train_atelectasis  = os.path.join(train_dir, 'atelectasis')\n","train_effusion     = os.path.join(train_dir, 'effusion')\n","train_infiltration = os.path.join(train_dir, 'infiltration')\n","train_mass         = os.path.join(train_dir, 'mass')\n","train_nodule       = os.path.join(train_dir, 'nodule')\n","train_none         = os.path.join(train_dir, 'none')\n","train_pneumothorax = os.path.join(train_dir, 'pneumothorax')\n","\n","# Validation Data\n","val_atelectasis  = os.path.join(validation_dir, 'atelectasis')\n","val_effusion     = os.path.join(validation_dir, 'effusion')\n","val_infiltration = os.path.join(validation_dir, 'infiltration')\n","val_mass         = os.path.join(validation_dir, 'mass')\n","val_nodule       = os.path.join(validation_dir, 'nodule')\n","val_none         = os.path.join(validation_dir, 'none')\n","val_pneumothorax = os.path.join(validation_dir, 'pneumothorax')\n","\n","# Test Data\n","test_atelectasis  = os.path.join(test_dir, 'atelectasis')\n","test_effusion     = os.path.join(test_dir, 'effusion')\n","test_infiltration = os.path.join(test_dir, 'infiltration')\n","test_mass         = os.path.join(test_dir, 'mass')\n","test_nodule       = os.path.join(test_dir, 'nodule')\n","test_none         = os.path.join(test_dir, 'none')\n","test_pneumothorax = os.path.join(test_dir, 'pneumothorax')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOZTZaUPblSc"},"source":["Use the code below to check how many images are available for each class for the 3 sets (training, validation and testing) of images."]},{"cell_type":"code","metadata":{"id":"OU87T7jxblpK"},"source":["print('Total training atelectasisat images:', len(os.listdir(train_atelectasis)))\n","print('Total training effusion images:', len(os.listdir(train_effusion)))\n","print('Total training infiltration images:', len(os.listdir(train_infiltration)))\n","print('Total training mass images:', len(os.listdir(train_mass)))\n","print('Total training nodule images:', len(os.listdir(train_nodule)))\n","print('Total training no finding images:', len(os.listdir(train_none)))\n","print('Total training pneumothorax images:', len(os.listdir(train_pneumothorax)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXxKsTapblvu"},"source":["print('Total validation atelectasisat images:', len(os.listdir(val_atelectasis)))\n","print('Total validation effusion images:', len(os.listdir(val_effusion)))\n","print('Total validation infiltration images:', len(os.listdir(val_infiltration)))\n","print('Total validation mass images:', len(os.listdir(val_mass)))\n","print('Total validation nodule images:', len(os.listdir(val_nodule)))\n","print('Total validation no finding images:', len(os.listdir(val_none)))\n","print('Total validation pneumothorax images:', len(os.listdir(val_pneumothorax)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyqIOWNkbl2O"},"source":["print('Total test atelectasisat images:', len(os.listdir(test_atelectasis)))\n","print('Total test effusion images:', len(os.listdir(test_effusion)))\n","print('Total test infiltration images:', len(os.listdir(test_infiltration)))\n","print('Total test mass images:', len(os.listdir(test_mass)))\n","print('Total test nodule images:', len(os.listdir(test_nodule)))\n","print('Total test no finding images:', len(os.listdir(test_none)))\n","print('Total test pneumothorax images:', len(os.listdir(test_pneumothorax)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gMqSRRYTbuGc"},"source":["### A. Build a CNN from scratch\n","Build a shallow (2-4 convolution layers) CNN. You are free to choose any values you wish for the filter size(s), pooling window size(s), and activation function(s). Please use an input shape of 64 x 64 and note that while these images look like greyscale images, they are in fact color images. Include a dense layer on top along with an appropriate output layer (number of neurons and activation function). Be sure to also include the `model.compile` function with an appropriate choice of loss function and performance metric."]},{"cell_type":"code","metadata":{"id":"GpeND796bucX"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vllkl4T6b0na"},"source":["#### Define the image generator\n","Using the `ImageDataGenerator` function, create `train_datagen` and `test_datagen` generators that rescale the images appropriately. Then define a training set generator and validation set generator using the generators `train_datagen` and `test_datagen` and the `.flow_from_directory` function. Specify the `target_size` (it should match the input size above), set the `batch_size` to 20 and choose an appropriate `class_mode`."]},{"cell_type":"code","metadata":{"id":"CgIlz05bb1IR"},"source":["# Your code here\n","from keras.preprocessing.image import ImageDataGenerator\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZ7tCXLwb6cV"},"source":["Use this code chunck to view the shapes of one batch of your training images and labels."]},{"cell_type":"code","metadata":{"id":"tqglg6-Pb6lI"},"source":["for data_batch, labels_batch in train_generator:\n","    print('data batch shape:', data_batch.shape)\n","    print('labels batch shape:', labels_batch.shape)\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s82-zKuxb-wV"},"source":["#### Compile your model\n","Be sure to choose appropriate numbers for the `steps_per_epoch` and `validation_steps` parameters. If one of the numbers is not a multiple of the batch size, round up to the nearest integer. Run this model for 30 epochs.\n","\n","**Note: the first epoch will take about 15-20 minutes to run. The rest of the epochs will only take about a minute each to run.**"]},{"cell_type":"code","metadata":{"id":"zF6sycaob-6z"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-deby4j-cOH8"},"source":["### Plot training and validation loss\n","Plot the training and validation loss. Does the model seem to be overfitting?\n"]},{"cell_type":"code","metadata":{"id":"c_t7xxyJcOgq"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-aUE1G5LcOp-"},"source":["### Test accuracy\n","Calculate and report the test set accuracy."]},{"cell_type":"code","metadata":{"id":"1QWgpKzEcOyf"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSdWmpURcO56"},"source":["### B. Using data augmentation\n","Using the same architecture above, fit the CNN using data augmentation. You are free to choose the type of transformations made to the training images, but keep `batch_size` equal to 20 and the `steps_per_epoch` the same as in part A. Be sure to include a `Dropout` layer before the first dense layer. Run this model for 30 epochs. **Note that this should take about an hour to run.**"]},{"cell_type":"markdown","metadata":{"id":"gmZINhYMcca1"},"source":["Check out a few examples of augmented images with the code below."]},{"cell_type":"code","metadata":{"id":"Uxbftu-ycckj"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","# This is module with image preprocessing utilities\n","from keras.preprocessing import image\n","\n","fnames = [os.path.join(train_nodule, fname) for fname in os.listdir(train_nodule)]\n","\n","# We pick one image to \"augment\"\n","img_path = fnames[3]\n","\n","# Read the image and resize it\n","img = image.load_img(img_path, target_size=(64, 64))\n","\n","# Convert it to a Numpy array with shape (64, 64, 3)\n","x = image.img_to_array(img)\n","\n","# Reshape it to (1, 64, 64, 3)\n","x = x.reshape((1,) + x.shape)\n","\n","# The .flow() command below generates batches of randomly transformed images.\n","# It will loop indefinitely, so we need to `break` the loop at some point!\n","i = 0\n","for batch in train_datagen.flow(x, batch_size=1):\n","    plt.figure(i)\n","    imgplot = plt.imshow(image.array_to_img(batch[0]))\n","    i += 1\n","    if i % 4 == 0:\n","        break\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxKGs-aBcPBA"},"source":["# Your code here (train_datagen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x30rraK5ckmL"},"source":["# Your code here (train_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6aVR4_Kckt6"},"source":["# Your code here (Train network)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upxywur8cq3r"},"source":["#### Plot training and validation loss\n","Plot the training and validation loss. Does the model seem to be overfitting? Did data augmentation do anything to mitigate overfitting of the original model?\n"]},{"cell_type":"code","metadata":{"id":"4JnUmUqLctT6"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gO4o9tAHcwQQ"},"source":["#### Test accuracy\n","Calculate and report the test set accuracy."]},{"cell_type":"code","metadata":{"id":"h7ZnxSWHctZW"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jcOQbByczYW"},"source":["### C. Using a pretrained CNN without data augmentation\n","\n","Use one of the pre-trained models in Keras that has been trained using the Imagenet data set as a convolutional base. Extract features by running your training set through the base. "]},{"cell_type":"code","metadata":{"id":"Hrim6OUYc114"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHWYuiKwc4dU"},"source":["Take this output and train a classifier. You may use the classifier from previous parts of this question."]},{"cell_type":"code","metadata":{"id":"t_vhLJOBc4yW"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59mXw8-ndGqZ"},"source":["#### Plot training and validation loss\n","Plot the training and validation loss. Does the model seem to be overfitting? Does this model improve on the original model?"]},{"cell_type":"code","metadata":{"id":"zt-YT4ZhdGyi"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOeTWuvFdG6E"},"source":["#### Test accuracy\n","Calculate and report the test set accuracy."]},{"cell_type":"code","metadata":{"id":"yH6o9efZdHA4"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpgqV1zgdHIa"},"source":["### D. Summarize results\n","- Summarize the results from the 3 models you built. Which model would you choose to make future predictions? \n","\n","\n","- List and describe at least 3 reasons for the poor performance of the models.\n","\n"]}]}